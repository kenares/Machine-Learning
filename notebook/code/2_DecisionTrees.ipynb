{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "In the past two Decision Tree Notebook, we learned about how decision trees are constructed. We used a modified version of <b>ID3</b>, which is a bit simpler than the most common tree building algorithms, [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm), and [CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees).\n",
    "However, the basics are all the same, and so we can apply the principles we learned about how decision trees work to any tree construction algorithm.\n",
    "\n",
    "1. <b>Using the Decision Tree with scikit_Learn</b>\n",
    "2. <b> Splitting the Data into Train and Test Sets </b>\n",
    "3. <b> Evaluating the error</b>\n",
    "4. <b> Computuing the error on the training set </b>\n",
    "5. <b> Decision Tree Overfitting </b>\n",
    "6. <b>Building A Shallower Tree</b>\n",
    "7. <b>More Parameter Tweaking</b>\n",
    "8. <b> Exploring Decision Tree Variance</b>\n",
    "9. <b> Pruning</b>\n",
    "10. <b>When To Use Decision Trees</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "root = r\"/Users/Kenneth-Aristide/anaconda3/bin/python_prog/ML/styles/bmh_matplotlibrc.json\"\n",
    "s = json.load(open(root))\n",
    "matplotlib.rcParams.update(s)\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_headers =['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation',\n",
    "         'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country']\n",
    "\n",
    "income = pickle.load(open(\"/Users/Kenneth-Aristide/anaconda3/bin/python_prog/ML/data/income.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Decision Trees With Scikit-Learn\n",
    "We can use scikit-Learn package to fit decision tree.\n",
    "We use the <i><b>DecisionTreeClassifier</b></i> class for classification problems and <i><b>DecisionTreeRegressor</b></i> for regression problems. Both of these classes are in the <i><b>sklearn.tree</b></i> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = _headers\n",
    "\n",
    "# Instantiate the classifier\n",
    "# set random_state to 1, to keep the result consistent\n",
    "clf = DecisionTreeClassifier(random_state = 1)\n",
    "\n",
    "clf.fit(income[features], income[\"high_income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClass:\n",
    "    def __init__(self, class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
    "                splitter ='best'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Convenience function:\n",
    "            initialize a DecisionTreeClassifier\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        #self.min_sample_leaf = min_sample_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def learn(self, X, y):\n",
    "        clf = DecisionTreeClassifier(max_depth = self.max_depth, min_samples_split=self.min_samples_split,\n",
    "                                     random_state = self.random_state)\n",
    "        clf.fit(X, y)\n",
    "        return clf\n",
    "        \n",
    "    def predict(self, clf, new_X):\n",
    "        predictions = clf.predict(new_X)\n",
    "        return predictions\n",
    "    \n",
    "    def compute_score(self, predictions, labels):\n",
    "        auc_score = roc_auc_score(labels, predictions)\n",
    "        return auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Train and Test Sets \n",
    "Now that we've fit a model, we can make predictions. We'll want to split our data into training and testing sets first. If we don't, we'll be making predictions on the same data that we train our algorithm with. This leads to overfitting, and will make our error appear lower than it is.<br>\n",
    "\n",
    "<b>Overfitting</b> is the first example, where you memorize the details of the training set, but are unable to generalize to new examples that you're asked to make predictions on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffled_index = np.random.permutation(income.index)\n",
    "shuffled_income = income.iloc[shuffled_index]\n",
    "\n",
    "split_line = math.floor(income.shape[0] * .8)\n",
    "train = shuffled_income[:split_line]\n",
    "test = shuffled_income[split_line:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the error\n",
    "While there are many methods to evaluate error with classification, we'll use [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_curve), which we covered extensively earlier in the machine learning material.\n",
    "\n",
    "AUC ranges from 0 to 1, and is ideal for binary classification. The higher the AUC, the more accurate our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741645009914\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 1)\n",
    "clf.fit(train[features], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[features])\n",
    "\n",
    "auc_score = roc_auc_score(test[\"high_income\"], predictions)\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computuing the error on the training set \n",
    "The AUC for the predictions on the testing set is about .694. Let's compare this against the AUC for predictions on the training set to see if the model is overfitting.\n",
    "\n",
    "It's normal for the model to predict the training set better than the testing set. After all, it has full knowledge of that data and the outcomes. However, if the AUC between training set predictions and actual values is <b>significantly higher</b> than the AUC between test set predictions and actual values, it's a sign that the model may be <i><b>overfitting</b></i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions = clf.predict(train[features])\n",
    "auc_score_train = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "auc_score_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Overfitting\n",
    "Our AUC on the training set was perfect. The AUC on the test set was 0.743. There's no hard and fast rule on when overfitting is happening, but our model is predicting the training set much better than it's predicting the test set. Splitting the data into training and testing sets doesn't prevent overfitting -- it just helps us detect it and fix it.\n",
    "\n",
    "Trees overfit when they have too much depth, and make overly complex rules that match the training data, but aren't able to generalize well to new data.\n",
    "\n",
    "This may seem to be a strange principle at first, but the more depth a tree has, typically the worse it performs on new data\n",
    "\n",
    "### Building A Shallower Tree\n",
    "There are three main ways to combat overfitting :\n",
    "\n",
    "1. <b>Prune</b> the tree after building to remove unneeded leaves\n",
    "2. Use <b>ensembling</b> to blend the predictions of many trees.\n",
    "3. Restrict the <b>depth</b> of the tree while you're building it.\n",
    "\n",
    "We'll explore all of these, but we'll look at the third method first.\n",
    "\n",
    "By controlling how deep the tree can go while we build it, we keep the rules more general than they would be otherwise. This prevents the tree from overfitting\n",
    "\n",
    "<i>\n",
    "We can restrict how deep the tree is built with a few parameters when we initialize the $DecisionTreeClassifier$ class:\n",
    "\n",
    "<b>max_depth</b> -- this globally restricts how deep the tree can go.\n",
    "\n",
    "<b>min_samples_split</b> -- The minimum number of rows needed in a node before it can be split. For example, if this is set to 2, then nodes with 2 rows won't be split, and will become leaves instead.\n",
    "\n",
    "<b>min_samples_leaf</b> -- the minimum number of rows that a leaf must have.\n",
    "\n",
    "<b>min_weight_fraction_leaf</b> -- the fraction of input rows that are required to be at a leaf.\n",
    "\n",
    "<b>max_leaf_nodes</b> -- the maximum number of total leaves. This will cap the count of leaf nodes as the tree is being built.\n",
    "\n",
    "As you can see, some of these parameters don't make sense together. Having max_depth and max_leaf_nodes together isn't allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9178262040397265 \n",
      " test_score 0.7594088171011247\n"
     ]
    }
   ],
   "source": [
    "min_samples_split = 13\n",
    "\n",
    "# Initialize the model\n",
    "model = DecisionTreeClass(min_samples_split = 13)\n",
    "\n",
    "# Learn, predict and compute score\n",
    "clf = model.learn(train[features], train['high_income'])\n",
    "test_predictions = model.predict(clf, test[features])\n",
    "auc_score_test = model.compute_score(test['high_income'], test_predictions)\n",
    "\n",
    "train_predictions = model.predict(clf, train[features])\n",
    "auc_score_train = model.compute_score(train['high_income'], train_predictions)\n",
    "\n",
    "print(\"train score {0} \\n test_score {1}\".format(auc_score_train, auc_score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Parameter Tweaking\n",
    "It seem we reduce a bit overfitting of our previous model.\n",
    "Now let's play with other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.8329212397511541 \n",
      " test_score 0.8196028773699573\n"
     ]
    }
   ],
   "source": [
    "min_samples_split = 13\n",
    "max_depth = 7\n",
    "random_state = 1\n",
    "\n",
    "# Initialize the model\n",
    "model = DecisionTreeClass(min_samples_split = min_samples_split, random_state = random_state, max_depth = max_depth)\n",
    "\n",
    "# Learn, predict and compute score\n",
    "clf = model.learn(train[features], train['high_income'])\n",
    "test_predictions = model.predict(clf, test[features])\n",
    "auc_score_test = model.compute_score(test['high_income'], test_predictions)\n",
    "\n",
    "train_predictions = model.predict(clf, train[features])\n",
    "auc_score_train = model.compute_score(train['high_income'], train_predictions)\n",
    "\n",
    "print(\"train score {0} \\n test_score {1}\".format(auc_score_train, auc_score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aren't overfitting anymore since both AUC valeus are about the same. Let's tweak the parameters more aggressively, and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.7953180123943951 \n",
      " test_score 0.7842609522908529\n"
     ]
    }
   ],
   "source": [
    "min_samples_split = 100\n",
    "max_depth = 2\n",
    "random_state = 1\n",
    "\n",
    "# Initialize the model\n",
    "model = DecisionTreeClass(min_samples_split = min_samples_split, random_state = random_state, max_depth = max_depth)\n",
    "\n",
    "# Learn, predict and compute score\n",
    "clf = model.learn(train[features], train['high_income'])\n",
    "test_predictions = model.predict(clf, test[features])\n",
    "auc_score_test = model.compute_score(test['high_income'], test_predictions)\n",
    "\n",
    "train_predictions = model.predict(clf, train[features])\n",
    "auc_score_train = model.compute_score(train['high_income'], train_predictions)\n",
    "\n",
    "print(\"train score {0} \\n test_score {1}\".format(auc_score_train, auc_score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy went down.\n",
    "\n",
    "This is because we're now underfitting. Underfitting is what happens when our model is too simple to actually explain the relations between the variables.\n",
    "\n",
    "### The Bias-Variance Tradeoff\n",
    "By artificially restricting the depth of our tree, we prevent it from creating a complex enough model to correctly categorize some of the rows. If we don't perform the artificial restrictions, the tree becomes too complex, and fits quirks in the data that only exist in the training set, but don't generalize to new data.\n",
    "\n",
    "This is known as the bias-variance tradeoff. If we take a random sample of training data and create many models, if the predictions of the models for the same row are far apart from each other, we have high variance. If we take a random sample of training data, and create many models, and the predictions of the models for the same row are close together, but far from the actual value, then we have high bias.\n",
    "\n",
    "High bias can cause underfitting -- if a model is consistently failing to predict the correct value, it may be that it is too simple to actually model the data.\n",
    "\n",
    "High variance can cause overfitting -- if a model is very susceptible to small changes in the input data, and changes its predictions massively, then it is likely fitting itself to quirks in the training data, and not making a generalizable model.\n",
    "\n",
    "It's called the bias-variance tradeoff because decreasing one will usually increase the other. This is a limitation of all machine learning algorithms.\n",
    "\n",
    "In general, decision trees suffer from high variance. The whole structure of a decision tree can change if you make a minor alteration to its training data. By restricting the depth of the tree, we increase the bias and decrease the variance. If we restrict the depth too much, we increase bias to the point where it will underfit.\n",
    "\n",
    "Generally, you'll need to use your intuition and manually tweak parameters to get the \"right\" fit.\n",
    "\n",
    "### Exploring Decision Tree Variance\n",
    "\n",
    "We can induce variance and see what happens with a decision tree. To add noise to the data, we'll just add a column of random values. A model with high variance (like a decision tree) will pick up on this noise, and overfit to it. This is because models with high variance are very sensitive to small changes in input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9999747129924645 \n",
      " test_score 0.7437527265252553\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "income['noise'] = np.random.randint(4, size = income.shape[0])\n",
    "_headers =['noise', 'age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation',\n",
    "         'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country']\n",
    "\n",
    "split_line = math.floor(income.shape[0] * .8)\n",
    "shuffled_index = np.random.permutation(income.index)\n",
    "shuffled_income = income.loc[shuffled_index]\n",
    "train = shuffled_income[:split_line]\n",
    "test = shuffled_income[split_line:]\n",
    "\n",
    "features = _headers[:-1]\n",
    "# Initialize the model\n",
    "model = DecisionTreeClass(random_state = 1)\n",
    "\n",
    "# Learn, predict and compute score\n",
    "clf = model.learn(train[features], train['high_income'])\n",
    "test_predictions = model.predict(clf, test[features])\n",
    "auc_score_test = model.compute_score(test['high_income'], test_predictions)\n",
    "\n",
    "train_predictions = model.predict(clf, train[features])\n",
    "auc_score_train = model.compute_score(train['high_income'], train_predictions)\n",
    "\n",
    "print(\"train score {0} \\n test_score {1}\".format(auc_score_train, auc_score_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "As you can see above, the random noise column causes significant overfitting. Our test set accuracy decreases to .743, and our training set accuracy increases to .999\n",
    "\n",
    "As you can see above, the random noise column causes significant overfitting. Our test set accuracy decreases to .691, and our training set accuracy increases to .975.\n",
    "\n",
    "One way to prevent overfitting that we tried before was to prevent the tree from growing beyond a certain depth. Another technique is called $pruning$. Pruning involves building a full tree, and then removing the leaves that don't add to prediction accuracy. Pruning prevents a model from becoming overly complex, and can make a simpler model with higher accuracy on the testing set.\n",
    "\n",
    "Pruning is less commonly used than <b>parameter optimization</b> (like we just did), and <b>ensembling</b>. That's not to say that it isn't an important technique, and we'll cover it in more depth down the line.\n",
    "\n",
    "\n",
    "### When To Use Decision Trees\n",
    "\n",
    "Let's go over the main advantages and disadvantages of decision trees. The main advantages of decision trees are:\n",
    "\n",
    "1. Easy to interpret\n",
    "2. Relatively fast to fit and make predictions\n",
    "3. Able to handle multiple types of data\n",
    "4. Can pick up nonlinearities in data, and are usually fairly accurate\n",
    "\n",
    "The main disadvantage is a tendency to overfit.\n",
    "\n",
    "<b>In tasks where it's important to be able to interpret and convey why the algorithm is doing what it's doing, decision trees are a good choice.</b>\n",
    "\n",
    "The most powerful way to reduce decision tree overfitting is to create ensembles of trees. A popular algorithm to do this is called random forest.\n",
    "In cases where prediction accuracy is the most important consideration, random forests usually perform better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
