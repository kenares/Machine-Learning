{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Kaggle competition\n",
    "\n",
    "#### Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import useful libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from IPython.core.pylabtools import figsize\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sns.set(style = \"darkgrid\", palette = \"muted\")\n",
    "pd.set_option(\"display.mpl_style\", \"default\")\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(root):\n",
    "    \"\"\"\n",
    "    Conveniance function:\n",
    "        import and clean titanic data\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(root)\n",
    "    titanic = data.copy()\n",
    "    \n",
    "    # thes variable can't help us predict the survived and there is too many nan for Cabin\n",
    "    titanic = titanic.drop([\"Cabin\", \"Ticket\"], axis = 1)\n",
    "    #del titanic[\"Cabin\"], titanic[\"Ticket\"]\n",
    "    \n",
    "    titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "    titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "    titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "    titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "    titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "    \n",
    "    #Notice if no NAN nothing will be done\n",
    "    titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "    titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(0)\n",
    "    titanic[\"Fare\"] = titanic[\"Fare\"].fillna(titanic[\"Fare\"].median())\n",
    "    \n",
    "    return titanic\n",
    "\n",
    "\n",
    "def get_title(name):\n",
    "    \"\"\"\n",
    "    Convenience function : \n",
    "        Get title, Mr. Mrs, Master form the name \n",
    "    \"\"\"\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_train = r\"/Users/Kenneth-Aristide/anaconda3/bin/python_prog/ML/data/train_titanic.csv\"\n",
    "root_test = r\"/Users/Kenneth-Aristide/anaconda3/bin/python_prog/ML/data/test_titanic.csv\"\n",
    "\n",
    "titanic = process_data(root_train)\n",
    "titanic_test = process_data(root_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple machine learning predictive model with sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression score : 0.7833894500561167\n"
     ]
    }
   ],
   "source": [
    "# On to Modelling, we will start with a simple linear regression with sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialise the Linear regression class\n",
    "LR = LinearRegression()\n",
    "\n",
    "# Generate Cross Validation folds returning row indices corresponding to train and test \n",
    "# We set random_state  to ensure we get the same split anytime we run this\n",
    "\n",
    "kf = KFold(titanic.shape[0], n_folds = 3, random_state = 1)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for train, test in kf:\n",
    "    \n",
    "    train_predictors = titanic[predictors].iloc[train, :] # take all columns\n",
    "    train_target = titanic[\"Survived\"].iloc[train] # just one\n",
    "    \n",
    "    # Training the algorithm LR\n",
    "    LR.fit(train_predictors, train_target)\n",
    "    \n",
    "    # make predictions on the test fold\n",
    "    test_predictions = LR.predict(titanic[predictors].iloc[test, :])\n",
    "    predictions.append(test_predictions)\n",
    "    \n",
    "\n",
    "# Let's evaluate the error, the error metric here is the percentage of correct prediction\n",
    "predictions = np.concatenate(predictions, axis = 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <= .5] = 0\n",
    "\n",
    "accuracy = np.sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(\"linear regression score : {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score :  0.7878787878787877\n"
     ]
    }
   ],
   "source": [
    "# Let's try with Logistic Regression to output value between 0 and 1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "\n",
    "#Initialize the algorithm\n",
    "LR = LogisticRegression(random_state = 1)\n",
    "\n",
    "# compute the score for all cross-validation folds, much simpler than before\n",
    "scores = cross_validation.cross_val_score(LR, titanic[predictors], titanic[\"Survived\"], cv = 3)\n",
    "print(\"Logistic Regression score :  {}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the a logistic regression algorithm and make a submission file with his result\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set\n",
    "_predictions = LR.predict(titanic_test[predictors])\n",
    "\n",
    "# Generate a first submission file\n",
    "submission = pd.DataFrame({\n",
    "        'PassengerID': titanic_test[\"PassengerId\"],\n",
    "        'Survived': _predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engeeniring\n",
    "Let's try to improve our score now by : \n",
    "1. trying to figure out what can be the best $features$ for the algorithm\n",
    "2. Use a better machine learning algorthim\n",
    "3. combine multiple machine learning algorithm : $Ensembling$\n",
    "\n",
    "### Random Forest\n",
    "<i>Random Forest</i> can pick $nonlinear$ tendencies in the data\n",
    "With random forest, we build hundreds of trees with slightly randomized input data, and slightly randomized split points. Each trees in a random forest gets a random subset of the overall training data. Each split point in each tree is performed on a random subset of the potential columns to split on.\n",
    "By avereging the predictions of all the trees, we get a strong overall prediction and minimize overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier score 0.8013468013468014\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with default parameter \n",
    "# n_estimators is the number of tree we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of sample we can have at the place where \n",
    "# a tree branch end (the bottom point of the tree)\n",
    "\n",
    "RFC = RandomForestClassifier(random_state = 1, n_estimators = 10, min_samples_split = 2, min_samples_leaf = 1)\n",
    "\n",
    "scores = cross_validation.cross_val_score(RFC, titanic[predictors], titanic[\"Survived\"], cv = 3)\n",
    "print(\"Random Forest Classifier score {}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest parameter tunning\n",
    "1. The first and easier way to improve the accuracy is to increase the number of tree we're using.\n",
    "2. Training more tree will take more time, but because we of the fact that we are averaging the many prediction made on different subsets of the data, having more tree will increase accuracy.\n",
    "3. Increase <b>min_samples_split</b> and <b>min_smples_leaf</b> can reduce overfitting.\n",
    "We will end up with a model that less overfit, and that can generalize better and it will actually perform better on unseen data but worse on seen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier with more tree score 0.8204264870931537\n"
     ]
    }
   ],
   "source": [
    "_RFC = RandomForestClassifier(random_state = 1, n_estimators = 50, min_samples_split = 4, min_samples_leaf = 2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(_RFC, titanic[predictors], titanic[\"Survived\"], cv = 3)\n",
    "print(\"Random Forest Classifier with more tree score {}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new features (predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "titanic[\"NameLenght\"] = titanic[\"Name\"].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Mlle          2\n",
      "Major         2\n",
      "Capt          1\n",
      "Don           1\n",
      "Countess      1\n",
      "Jonkheer      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Mme           1\n",
      "Sir           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get all the titles and print how often each occurs\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \n",
    "                 \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "\n",
    "# Verify that we converted everything well\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best features\n",
    "Features engineering is the most important part of any machine learning task, and there is a lot of more feature we could calculate. But we also need a way to figure out which feature is the best.\n",
    "\n",
    "One way to do this is to use $univariate$ $feature$ $selection$, this essentially goes coulumn by column and figure out which column correlate most closely with what we are trying to predict.\n",
    "\n",
    "$SelectKBest$ from sklearn elect the best feature from data, and allows to specify how many it select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr0AAAGXCAYAAAC+++gVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIonegEAKJ7oBQCgeKIXAIDi/T+BK5wBbwO7fQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c7bcd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.81705948372615034"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform feature selection\n",
    "selector = SelectKBest(f_classif, k = 5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "plt.bar(np.arange(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show();\n",
    "\n",
    "# pick the only the best predictors\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "_RFC_ = RandomForestClassifier(random_state = 1, n_estimators = 50, min_samples_split = 8, min_samples_leaf = 4)\n",
    "_scores = cross_validation.cross_val_score(_RFC_, titanic[predictors], titanic[\"Survived\"], cv = 3)\n",
    "_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "Another method that builds on a decision trees is $Gradient$ $Boosting$ $Classifier$ : \n",
    "    $Boosting$ involves training a ecision trees one after another, and feeding the $errors$ form trees into the next tree.\n",
    "So each tree is building on all the other trees that cames before it. Tjis can lead to overfitting if we build to many trees, though. \n",
    "\n",
    "Another way to limit overfitting is to limit the depth to which each tree in the gradient boosting procees can be build. We'll limit the tree depth to 3 to avoid overfitting.\n",
    "\n",
    "We'll try boosting instead of our random forest approach and see if we can improve our accuracy.\n",
    "\n",
    "### Ensembling\n",
    "One thing we can do to improve the accuracy of our predictions is to ensemble different classifiers. $Ensembling$ means that we generate predictions using information from a set of classifiers, instead of just one. In practice, this means that we average their predictions.\n",
    "\n",
    "Generally, the more diverse the models we ensemble, the higher our accuracy will be. Diversity means that the models generate their results from different columns, or use a very different method to generate predictions. Ensembling a random forest classifier with a decision tree probably won't work extremely well, because they are very similar. On the other hand, ensembling a linear regression with a random forest can work very well.\n",
    "\n",
    "One caveat with ensembling is that the classifiers we use have to be about the same in terms of accuracy. Ensembling one classifier that is much worse than another probably will make the final result worse.\n",
    "\n",
    "In this case, we'll ensemble logistic regression trained on the most linear predictors (the ones that have a linear ordering, and some correlation to Survived), and a gradient boosted tree trained on all of the predictors.\n",
    "\n",
    "we'll average the raw probabilities (from 0 to 1) that we get from our classifiers, and then assume that anything above .5 maps to one, and anything below or equal to .5 maps to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the \n",
    "#logistic regression, and everything with the gradient boosting classifier.\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), \n",
    "     [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First, we'll add titles to the test set.\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3,\n",
    "                 \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \n",
    "                 \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pd.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family ids.\n",
    "# We'll use the same ids that we did earlier.\n",
    "#print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint : \n",
    "    1. Try the random forest classifier in the ensemble\n",
    "    2. A support Vector machine might work well with this data\n",
    "    3. We could try a Neural Network\n",
    "    4. Boosting with different base classifier might work better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
