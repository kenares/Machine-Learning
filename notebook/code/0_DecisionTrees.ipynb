{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "Decision trees are a powerful and commonly used machine learning technique. The basic concept is very similar to trees you may have seen commonly used to aid decision-making. Here's an example:<br>\n",
    "<img src=\"http://localhost:8889/files/python_prog/ML/img/tree_0.png\", width = 350/>\n",
    "Trees can be used for classification or regression problems. In this lesson, we'll walk through the building blocks of making a decision tree automatically.<br>\n",
    "\n",
    "The decision tree algorithm is a supervised learning algorithm -- we first construct the tree with historical data, and then use it to predict an outcome. One of the major advantages of decision trees is that they can pick up nonlinear interactions between variables in the data that linear regression can't. In our bear wrestling example, a decision tree can pick up on the fact that you should only run away from large bears when escape is impossible, whereas a linear regression would have had to weight both factors in the absence of the other.<br><br>\n",
    "We'll be looking at individual income in the United States. The data is from the 1994 Census, and contains information on an individual's marital status, age, type of work, and more. The target column, or what we want to predict, is if they make less than or equal to 50k a year, or more than 50k a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "root = r\"/Users/Kenneth-Aristide/anaconda3/bin/python_prog/ML/styles/bmh_matplotlibrc.json\"\n",
    "s = json.load(open(root))\n",
    "matplotlib.rcParams.update(s)\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_categorical(label):\n",
    "    \"\"\"\n",
    "    Convenience function:\n",
    "        convert to pd.Categorical \n",
    "    \"\"\"\n",
    "    col = pd.Categorical.from_array(income[label])\n",
    "    income[label] = col.codes\n",
    "    \n",
    "    \n",
    "def compute_entropy(column):\n",
    "    \"\"\"\n",
    "    Convenience function:\n",
    "        Calculate entropy given a pandas Series, list, or numpy array\n",
    "    \"\"\"\n",
    "    counts = np.bincount(column)\n",
    "    probabilities = counts / len(column)\n",
    "    \n",
    "    entropy = 0\n",
    "    \n",
    "    for prob in probabilities:\n",
    "        if prob > 0 : \n",
    "            entropy += prob * math.log(prob, 2)\n",
    "        \n",
    "    return -entropy\n",
    "\n",
    "def compute_information_gain(data, split_name, target_name):\n",
    "    \"\"\"\n",
    "    Calculate information gain given a dataset, column to split on, and target\n",
    "    \"\"\"\n",
    "    total_entropy = compute_entropy(data[target_name])\n",
    "    \n",
    "    column = data[split_name]\n",
    "    median = column.median()\n",
    "    \n",
    "    left_split = data[column <= median]\n",
    "    right_split = data[column > median]\n",
    "    \n",
    "    to_subtract = 0\n",
    "    \n",
    "    for subset in [left_split, right_split]:\n",
    "        prob = (subset.shape[0] / data.shape[0])\n",
    "        to_subtract += prob * compute_entropy(subset[target_name])\n",
    "        \n",
    "    return total_entropy - to_subtract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "1. <b>Read the data</b>\n",
    "2. <b>Convert categorical variables</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>high_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age      workclass  fnlwgt    education  education_num  \\\n",
       "32556   27        Private  257302   Assoc-acdm             12   \n",
       "32557   40        Private  154374      HS-grad              9   \n",
       "32558   58        Private  151910      HS-grad              9   \n",
       "32559   22        Private  201490      HS-grad              9   \n",
       "32560   52   Self-emp-inc  287927      HS-grad              9   \n",
       "\n",
       "            marital_status          occupation relationship    race      sex  \\\n",
       "32556   Married-civ-spouse        Tech-support         Wife   White   Female   \n",
       "32557   Married-civ-spouse   Machine-op-inspct      Husband   White     Male   \n",
       "32558              Widowed        Adm-clerical    Unmarried   White   Female   \n",
       "32559        Never-married        Adm-clerical    Own-child   White     Male   \n",
       "32560   Married-civ-spouse     Exec-managerial         Wife   White   Female   \n",
       "\n",
       "       capital_gain  capital_loss  hours_per_week  native_country high_income  \n",
       "32556             0             0              38   United-States       <=50K  \n",
       "32557             0             0              40   United-States        >50K  \n",
       "32558             0             0              40   United-States       <=50K  \n",
       "32559             0             0              20   United-States       <=50K  \n",
       "32560         15024             0              40   United-States        >50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers =['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation',\n",
    "         'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "         'high_income']\n",
    "income = pd.read_csv(\"/Users/Kenneth-Aristide/anaconda3/bin/python_prog/ML/data/adult.csv\", names = headers,\n",
    "                    index_col = False)\n",
    "# set index_col = False to avoid pandas thinking the first column (age) is row indexes\n",
    "income.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "convert_to_categorical(\"workclass\")\n",
    "convert_to_categorical(\"education\")\n",
    "convert_to_categorical(\"marital_status\")\n",
    "convert_to_categorical(\"occupation\")\n",
    "convert_to_categorical(\"relationship\")\n",
    "convert_to_categorical(\"race\")\n",
    "convert_to_categorical(\"sex\")\n",
    "convert_to_categorical(\"native_country\")\n",
    "convert_to_categorical(\"high_income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split points\n",
    "A decision tree is constructed through a series of $nodes$ and $branches$. A node is where we split the data based on a variable, and a branch is one side of the split. example : <br>\n",
    "<img src=\"http://localhost:8889/files/python_prog/ML/img/tree_1.png\", width = 350/><br>\n",
    "In the above diagram, the node is splitting the data based on if the individual works in the private sector. There are two branches, No, and Yes. The workclass column indicates the sector people work in. The value Private in that column has been mapped to the numeric code 4 (we can check this by comparing the values in the workclass column that used to be labelled Private with the current values to see where they line up). So the No branch corresponds to workclass != 4, and the Yes branch corresponds to workclass == 4.\n",
    "\n",
    "This is exactly how a decision tree works -- we keep splitting the data based on variables. As we do this, the tree gets deeper and deeper. The tree we made above is two levels deep, because it has one split, and two \"levels\" of nodes.\n",
    "\n",
    "The tree below is 3 levels deep.\n",
    "<img src=\"/files/python_prog/ML/img/tree_2.png\", width = 350/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22696\n",
      "9865\n"
     ]
    }
   ],
   "source": [
    "privates_incomes = income[income.workclass ==  4]\n",
    "public_incomes = income[income.workclass != 4]\n",
    "print(len(privates_incomes))\n",
    "print(len(public_incomes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>When we performed the split, 9865 rows went to the left, where workclass does not equal 4, and 22696 rows went to the right, where workclass equals 4.<br>\n",
    "\n",
    "It's useful to think of a decision tree as a flow of rows of data. When we make a split, some rows will go to the right, and some rows will go to the left. As we build the tree deeper and deeper, each node will \"receive\" less and less rows.<br>\n",
    "\n",
    "Here's a look at the splits, and the number of rows that will exist at each node:\n",
    "<img src=\"/files/python_prog/ML/img/tree_3.png\", width = 350/>\n",
    "\n",
    "\n",
    "### Rationale behind splitting\n",
    "The nodes at the bottom of the tree, when we decide to stop splitting, are called <b><i>terminal nodes</i></b>, or <i><b>leaves</b><i/>. When we do our splits, we aren't doing them randomly -- we have an objective. Our goal is to ensure that we can make a prediction on future data. In order to do this, all rows in each leaf must have only one value for our target column.\n",
    "\n",
    "We're trying to predict the <b>high_income</b> column.\n",
    "If high_income is 1, it means that the person has an income higher than 50k a year.\n",
    "If high_income is 0, it means that they have an income less than or equal to 50k a year.\n",
    "\n",
    "After constructing a tree using the income data, we'll want to make predictions. In order to do this, we'll take a new row, and feed it through our decision tree. We'll check to see if the person works in the private sector. If they do, we'll check to see if they are native to the US, and so on.\n",
    "\n",
    "We'll eventually reach a leaf. The leaf will tell us what value we should predict for high_income.\n",
    "\n",
    "In order to be able to make this prediction, all leaves need to have rows with only a single value for high_income. So leaves can't have both 0 and 1 values in the high_income column. Each leaf can only have rows with the same values for our target. If this doesn't happen, we won't be able to make effective predictions.\n",
    "\n",
    "So, we keep splitting nodes until we get to a point where all the rows in a node have the same value for high_income.\n",
    "\n",
    "### Entropy\n",
    "Now that we have a high-level view of how decision trees work, let's get into the details and figure out how to perform the splits.\n",
    "\n",
    "In order to do this, we'll use a measure to figure out which variable to split a node on. Post-split, we'll have two datasets, each containing the rows from one side of the split.\n",
    "\n",
    "Since we're trying to get to leaves consisting of only 1s or only 0s for high_income, each split will need to get us closer to that goal.\n",
    "\n",
    "When we split, we'll try to separate as many 0s from 1s in the high_income column as we can. In order to do this, we need a metric for how \"together\" the different values in the high_income column are.\n",
    "\n",
    "A commonly used metric is called entropy. Entropy refers to disorder. The more \"mixed together\" 1s and 0s are, the higher the entropy. A dataset consisting of only 1s in the high_income column would have low entropy.\n",
    "\n",
    "Entropy, which is not to be confused with entropy from physics, comes from information theory. Information theory is based on probability and statistics, and deals with the transmission, processing, utilization, and extraction of information. A key concept in this is the notion of a bit of information. One bit of information is one unit of information. It can be represented as a binary number -- a bit of information either has the value 1, or 0. If there's an equal probability of tomorrow being sunny (1) and tomorrow being not sunny (0), if I tell you that it will be be sunny, I've given you one bit of information.\n",
    "\n",
    "Entropy can also be thought of in terms of information. If we flip a coin where both sides are heads, we know upfront that the result will be heads. Thus, we gain no new information by flipping the coin, so entropy is 0. On the other hand, if the coin has a heads side and a tails side, there's a 50% probability of landing on either. Thus, flipping the coin gives us one bit of information -- which side the coin landed on. There's much more complexity, especially when you get to cases with more than two possible outcomes, or differential probabilities. However, a deep understanding of entropy isn't necessary for constructing decision trees.\n",
    "\n",
    "\n",
    "The formula for $entropy$ is :\n",
    "$$- \\sum_{i=1}^c P(x_i)\\log_{b}(P(x_i))$$\n",
    "\n",
    "example : \n",
    "<img src=\"http://localhost:8889/files/python_prog/ML/img/tree_4.png\", width = 150/>\n",
    "Entropy = $-((2 / 5 * \\log_{2}(2/5)) + (3/5 * \\log_{2}(3/5))) = .97$\n",
    "\n",
    "We get less than one \"bit\" of information -- we only get .97. This is because there are slightly more 1s in the sample data than 0s. This means that if we were predicting a new value, we could guess that the answer is 1, and be right more often than we're wrong (there's a .6 probability of the answer being 1). Because of this prior knowledge, we gain less than a full \"bit\" of information when we observe a new value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7963839552022132\n"
     ]
    }
   ],
   "source": [
    "# Entropy of the high_income\n",
    "prob_0 = len(income[income.high_income == 0])/len(income.high_income)\n",
    "prob_1 = len(income[income.high_income == 1])/len(income.high_income)\n",
    "\n",
    "income_entropy = -(prob_0 * math.log(prob_0, 2) + prob_1 * math.log(prob_1, 2))\n",
    "print(income_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "We'll need a way to go from computing entropy to figuring out which variable to split based on. We can do this using <b><i>information gain</i></b>.<br> Information gain tells us which split will reduce entropy the most.\n",
    "\n",
    "Here's the formula for information gain:\n",
    "$$IG(T, A) = Entropy(T) - \\sum_{v\\in A}\\frac{T_{v}}{T} . Entropy(T_v)$$\n",
    "\n",
    "This may look complicated, but we'll break it down. We're computing information gain $(IG)$ for a given target variable ($T$), and a given variable that we want to split based on ($A$). To compute this, we first calculate the entropy for T. Then, for each unique value v in the variable A, we compute the number of rows in which A takes on the value v, and divide it by the total number of rows. We then multiply this by the entropy of the rows where A is v.\n",
    "We add together all of these subset entropies, and subtract from the overall entropy to get information gain.\n",
    "\n",
    "For an even simpler explanation, we're finding the entropy of each set post-split, weighting it by the number of items in each split, then subtracting from the current entropy. If it's positive, we've lowered entropy with our split. The higher it is, the more we've lowered entropy.\n",
    "\n",
    "One strategy to construct trees is to create as many branches at each node as there are unique values for the variable being split on. So if the variable has 3 or 4 values, that would result in 3 or 4 branches. Usually, this is more complexity than it's worth, and doesn't improve prediction accuracy, but it's worth knowing about.\n",
    "\n",
    "To simplify the calculation of information gain, and to make splits simpler, we won't do it for each unique value. Instead, for the variable we're splitting on, we'll find the median. Any rows where the value of the variable is below the median will split left, and the rest of the rows will split right. To compute information gain, we'll only have to compute entropies for two subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0470286613047\n"
     ]
    }
   ],
   "source": [
    "# compute the IG for splitting on the age column of income : \n",
    "median_age = income.age.median()\n",
    "left_split = income[income.age <= median_age]\n",
    "right_split = income[income.age > median_age]\n",
    "\n",
    "age_information_gain = income_entropy - \\\n",
    "(((left_split.shape[0] / income.shape[0]) * compute_entropy(left_split[\"high_income\"])) + \\\n",
    "(right_split.shape[0] / income.shape[0]) * compute_entropy(right_split[\"high_income\"]))\n",
    "\n",
    "print(age_information_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding The Best Split\n",
    "Now that we know how to compute information gain, we can compute the best variable to split a node on. When we start our tree, we want to make an initial split. We'll find the variable to split on by finding which split would have the highest information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marital_status\n"
     ]
    }
   ],
   "source": [
    "_headers =['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation',\n",
    "         'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country']\n",
    "information_gain = {}\n",
    "for col_name in _headers : \n",
    "    information_gain[col_name] = compute_information_gain(income, col_name, \"high_income\")\n",
    "    \n",
    "print(max(information_gain, key = information_gain.get))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've been following the <b>$ID3$ $algorithm$</b> to construct <i>decision trees</i>. There are other algorithms, including <b>$CART$</b> that use other measures for the split criterion. We'll learn more about these other algorithms in future.\n",
    "\n",
    "Next, we'll figure out how to recursively construct an entire tree, using the ID3 algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
